\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}

\title{Benchmarking Large Language Model Inference: Comparing Triton, TensorRT-LLM, and vLLM on Llama 3 and Granite 3.1}

\author{Anonymous Author(s)\\
Affiliation\\
Email: author@example.com}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large Language Model (LLM) inference is a crucial aspect of deploying AI systems efficiently. In this paper, we benchmark the inference performance of three popular inference engines—Triton, TensorRT-LLM, and vLLM—on two leading LLM architectures: Llama 3 and Granite 3.1. Our evaluation includes latency, throughput, memory consumption, and scalability across different hardware configurations. The results provide insights into the trade-offs among these frameworks, guiding AI practitioners in selecting the most suitable solution for their needs.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have seen widespread adoption, with their performance heavily influenced by inference efficiency. Selecting the right inference framework is essential to maximize throughput and minimize latency. In this paper, we conduct a systematic benchmarking of Triton, TensorRT-LLM, and vLLM on Llama 3 and Granite 3.1.

\section{Related Work}
Previous studies have examined inference optimizations for transformer-based models. NVIDIA's TensorRT-LLM has been optimized for GPU inference, while Triton provides flexibility across hardware, and vLLM introduces paged attention for improved memory efficiency. However, a direct comparison across these frameworks on the latest models is lacking.

\section{Methodology}
We evaluate the inference performance based on the following metrics:
\begin{itemize}
    \item \textbf{Latency:} Time taken to generate a response per token.
    \item \textbf{Throughput:} Number of tokens processed per second.
    \item \textbf{Memory Utilization:} GPU memory consumption under different loads.
    \item \textbf{Scalability:} Performance across varying batch sizes and hardware configurations.
\end{itemize}

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Hardware:} We use A100 and H100 GPUs for evaluation.
    \item \textbf{Models:} Llama 3 (70B) and Granite 3.1.
    \item \textbf{Software:} TensorRT-LLM vX.Y, Triton Inference Server vX.Y, vLLM vX.Y.
\end{itemize}

\section{Results}
\subsection{Latency Comparison}
We measure token generation latency across different batch sizes. Figure~\ref{fig:latency} shows the results.

\subsection{Throughput Comparison}
Table~\ref{tab:throughput} compares the token throughput across frameworks.

\subsection{Memory Utilization}
We analyze memory usage across different loads.

\section{Discussion}
The results indicate trade-offs between latency and throughput. TensorRT-LLM excels in optimized GPU performance, Triton provides a flexible deployment solution, and vLLM achieves high throughput through memory-efficient optimizations.

\section{Conclusion}
Our benchmarking study highlights the strengths and weaknesses of each inference engine for Llama 3 and Granite 3.1. Future work includes extending the evaluation to additional hardware and model architectures.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
