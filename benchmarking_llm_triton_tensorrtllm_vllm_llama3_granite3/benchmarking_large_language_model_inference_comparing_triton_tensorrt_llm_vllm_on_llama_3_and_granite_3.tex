\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

%SetFonts

%SetFonts


\title{Benchmarking Large Language Model Inference on Kubernetes: Comparing Triton, TensorRT-LLM, and vLLM on Llama 3.1 and Granite 3.1}
\author{C. Kang, C. Latschkowski, D. Marcus}
\date{02/27/2025}							% Activate to display a given date or no date

\begin{document}
\maketitle

\begin{abstract}
Large Language Model (LLM) inference is a crucial aspect of deploying AI systems efficiently. In this paper, we benchmark the inference performance of three popular inference engines—Triton, TensorRT-LLM, and vLLM—on two leading LLM architectures: Llama 3.1 and Granite 3.1. Our evaluation includes latency, throughput, memory consumption, and scalability across two different platforms, Linux, Kubernetes configurations. The results provide insights and optional performance configurations into the trade-offs among these frameworks, guiding AI practitioners in selecting the most suitable solution for their needs.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have seen widespread adoption, with their performance heavily influenced by inference efficiency. Combining the right combination of inference framework, model architecture and platform is essential to maximize throughput and minimize latency. In this paper, we conduct a systematic benchmarking of Triton, TensorRT-LLM, and vLLM with Llama 3 and Granite 3.1 on Linux and Kubernetes.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Model & Framework & Platform & Latency (ms) & Throughput (tokens/sec) \\
        \hline
        Llama-3.1-8B-Instruct & Triton & Linux & 50 & 200 \\
        Llama-3.1-8B-Instruct & TensorRT-LLM & Linux & 50 & 200 \\
        Llama-3.1-8B-Instruct & vLLM & Linux & 50 & 200 \\
        Granite-3.1-8B-Instruct & Triton & Kubernetes & 50 & 200 \\
        Granite-3.1-8B-Instruct & TensorRT-LLM & Kubernetes & 50 & 200 \\
        Granite-3.1-8B-Instruct & vLLM & Kubernetes & 50 & 200 \\
        \hline
    \end{tabular}
    \caption{Inference performance comparison.}
    \label{tab:performance}
\end{table}

\section{Related Work}
Previous studies have examined inference optimizations for transformer-based models. NVIDIA's TensorRT-LLM has been optimized for GPU inference, while Triton provides flexibility across hardware, and vLLM introduces paged attention for improved memory efficiency. However, a direct comparison across these frameworks on the latest models is lacking.

\section{Methodology}
We evaluate the inference performance based on the following metrics:
\begin{itemize}
    \item \textbf{Latency:} Time taken to generate a response per token.
    \item \textbf{Throughput:} Number of tokens processed per second.
    \item \textbf{Memory Utilization:} GPU memory consumption under different loads.
    \item \textbf{Scalability:} Performance across varying batch sizes and hardware configurations.
\end{itemize}

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Hardware:} We use A100 and H100 GPUs for evaluation.
    \item \textbf{Models:} Llama 3 (70B) and Granite 3.1.
    \item \textbf{Software:} TensorRT-LLM vX.Y, Triton Inference Server vX.Y, vLLM vX.Y.
\end{itemize}

\section{Results}
\subsection{Latency Comparison}
We measure token generation latency across different batch sizes. Figure~\ref{fig:latency} shows the results.

\subsection{Throughput Comparison}
Table~\ref{tab:throughput} compares the token throughput across frameworks.

\subsection{Memory Utilization}
We analyze memory usage across different loads.

\section{Discussion}
The results indicate trade-offs between latency and throughput. TensorRT-LLM excels in optimized GPU performance, Triton provides a flexible deployment solution, and vLLM achieves high throughput through memory-efficient optimizations.

\section{Conclusion}
Our benchmarking study highlights the strengths and weaknesses of each inference engine for Llama 3 and Granite 3.1. Future work includes extending the evaluation to additional hardware and model architectures.

\bibliographystyle{unsrt}
\bibliography{references}

% ================

\end{document}  